{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Code Review\n",
    "# GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Download the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "!cd data\n",
    "!echo \"Start downloading ...\"\n",
    "!wget https://s3.eu-central-1.amazonaws.com/avg-projects/giraffe/data/comprehensive_cars.zip\n",
    "!echo \"done! Start unzipping ...\"\n",
    "!unzip comprehensive_cars.zip\n",
    "!echo \"done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GIRAFFE the model that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIRAFFE(nn.Module):\n",
    "    ''' GIRAFFE model class.\n",
    "\n",
    "    Args:\n",
    "        device (device): torch device\n",
    "        discriminator (nn.Module): discriminator network\n",
    "        generator (nn.Module): generator network\n",
    "        generator_test (nn.Module): generator_test network\n",
    "    '''\n",
    "\n",
    "    def __init__(self, device=None,\n",
    "                 discriminator=None, generator=None, generator_test=None,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        if discriminator is not None:\n",
    "            self.discriminator = discriminator.to(device)\n",
    "        else:\n",
    "            self.discriminator = None\n",
    "        if generator is not None:\n",
    "            self.generator = generator.to(device)\n",
    "        else:\n",
    "            self.generator = None\n",
    "\n",
    "        if generator_test is not None:\n",
    "            self.generator_test = generator_test.to(device)\n",
    "        else:\n",
    "            self.generator_test = None\n",
    "\n",
    "    def forward(self, batch_size, **kwargs):\n",
    "        gen = self.generator_test\n",
    "        if gen is None:\n",
    "            gen = self.generator\n",
    "        return gen(batch_size=batch_size)\n",
    "\n",
    "    def generate_test_images(self):\n",
    "        gen = self.generator_test\n",
    "        if gen is None:\n",
    "            gen = self.generator\n",
    "        return gen()\n",
    "\n",
    "    def to(self, device):\n",
    "        ''' Puts the model to the device.\n",
    "\n",
    "        Args:\n",
    "            device (device): pytorch device\n",
    "        '''\n",
    "        model = super().to(device)\n",
    "        model._device = device\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a train step\n",
    "### - freeze the discrimninator to learn the generator\n",
    "### - freeze the generator to learn the discrimninator\n",
    "### NOTE: the input of  the generator is a list of 4 randoms vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(self, data, it=None):\n",
    "    ''' Performs a training step.\n",
    "\n",
    "    Args:\n",
    "        data (dict): data dictionary\n",
    "        it (int): training iteration\n",
    "    '''\n",
    "    loss_g = self.train_step_generator(data, it)\n",
    "    loss_d, reg_d, fake_d, real_d = self.train_step_discriminator(data, it)\n",
    "    return {\n",
    "        'generator': loss_g,\n",
    "        'discriminator': loss_d,\n",
    "        'regularizer': reg_d,\n",
    "    }\n",
    "\n",
    "def train_step_generator(self, data, it=None, z=None):\n",
    "    generator = self.generator\n",
    "    discriminator = self.discriminator\n",
    "\n",
    "    toggle_grad(generator, True)\n",
    "    toggle_grad(discriminator, False)\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    self.optimizer.zero_grad()\n",
    "\n",
    "    if self.multi_gpu:\n",
    "        latents = generator.module.get_vis_dict()\n",
    "        x_fake = generator(**latents)\n",
    "    else:\n",
    "        x_fake = generator()\n",
    "\n",
    "    d_fake = discriminator(x_fake)\n",
    "    gloss = compute_bce(d_fake, 1)\n",
    "\n",
    "    gloss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    if self.generator_test is not None:\n",
    "        update_average(self.generator_test, generator, beta=0.999)\n",
    "\n",
    "    return gloss.item()\n",
    "\n",
    "def train_step_discriminator(self, data, it=None, z=None):\n",
    "    generator = self.generator\n",
    "    discriminator = self.discriminator\n",
    "    toggle_grad(generator, False)\n",
    "    toggle_grad(discriminator, True)\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    self.optimizer_d.zero_grad()\n",
    "\n",
    "    x_real = data.get('image').to(self.device)\n",
    "    loss_d_full = 0.\n",
    "\n",
    "    x_real.requires_grad_()\n",
    "    d_real = discriminator(x_real)\n",
    "\n",
    "    d_loss_real = compute_bce(d_real, 1)\n",
    "    loss_d_full += d_loss_real\n",
    "\n",
    "    reg = 10. * compute_grad2(d_real, x_real).mean()\n",
    "    loss_d_full += reg\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if self.multi_gpu:\n",
    "            latents = generator.module.get_vis_dict()\n",
    "            x_fake = generator(**latents)\n",
    "        else:\n",
    "            x_fake = generator()\n",
    "\n",
    "    x_fake.requires_grad_()\n",
    "    d_fake = discriminator(x_fake)\n",
    "\n",
    "    d_loss_fake = compute_bce(d_fake, 0)\n",
    "    loss_d_full += d_loss_fake\n",
    "\n",
    "    loss_d_full.backward()\n",
    "    self.optimizer_d.step()\n",
    "\n",
    "    d_loss = (d_loss_fake + d_loss_real)\n",
    "\n",
    "    return (\n",
    "        d_loss.item(), reg.item(), d_loss_fake.item(), d_loss_real.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the input of the generator is a list of 4 randoms vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vis_dict(self, batch_size=32):\n",
    "    vis_dict = {\n",
    "        'batch_size': batch_size,\n",
    "        'latent_codes': self.get_latent_codes(batch_size),\n",
    "        'camera_matrices': self.get_random_camera(batch_size),\n",
    "        'transformations': self.get_random_transformations(batch_size),\n",
    "        'bg_rotation': self.get_random_bg_rotation(batch_size)\n",
    "    }\n",
    "    return vis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'get_latent_codes'\n",
    "z_shape_obj.shape\n",
    "Out[8]: torch.Size([16, 1, 256])\n",
    "z_app_obj.shape\n",
    "Out[9]: torch.Size([16, 1, 256])\n",
    "z_shape_bg.shape\n",
    "Out[10]: torch.Size([16, 128])\n",
    "z_app_bg.shape\n",
    "Out[11]: torch.Size([16, 128])\n",
    "    \n",
    "\n",
    "'get_random_camera'\n",
    "[[0.0875, 0.0000, 0.0000, -0.0000],\n",
    "[0.0000, 0.0875, 0.0000, -0.0000],\n",
    "[0.0000, 0.0000, 1.0000, -0.0000],\n",
    "[0.0000, 0.0000, 0.0000, 1.0000]]], device='cuda:0')\n",
    "camera_mat.shape\n",
    "Out[16]: torch.Size([16, 4, 4])\n",
    "    \n",
    "\n",
    "'get_random_transformations'\n",
    "s.shape\n",
    "Out[37]: torch.Size([16, 1, 3])\n",
    "\n",
    "[[0.2179, 0.1743, 0.1743]]])\n",
    "t.shape\n",
    "Out[38]: torch.Size([16, 1, 3])\n",
    "\n",
    "[[ 0.0061, -0.0880, 0.0000]]])\n",
    "R.shape\n",
    "Out[39]: torch.Size([16, 1, 3, 3])\n",
    "\n",
    "[[[ 0.6894, -0.7243,  0.0000],\n",
    "[ 0.7243,  0.6894,  0.0000],\n",
    "[ 0.0000,  0.0000,  1.0000]]]], device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Define the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DCDiscriminator(nn.Module):\n",
    "    ''' DC Discriminator class.\n",
    "\n",
    "    Args:\n",
    "        in_dim (int): input dimension\n",
    "        n_feat (int): features of final hidden layer\n",
    "        img_size (int): input image size\n",
    "    '''\n",
    "    def __init__(self, in_dim=3, n_feat=512, img_size=64):\n",
    "        super(DCDiscriminator, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        n_layers = int(log2(img_size) - 2)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [nn.Conv2d(\n",
    "                in_dim,\n",
    "                int(n_feat / (2 ** (n_layers - 1))),\n",
    "                4, 2, 1, bias=False)] + [nn.Conv2d(\n",
    "                    int(n_feat / (2 ** (n_layers - i))),\n",
    "                    int(n_feat / (2 ** (n_layers - 1 - i))),\n",
    "                    4, 2, 1, bias=False) for i in range(1, n_layers)])\n",
    "\n",
    "        self.conv_out = nn.Conv2d(n_feat, 1, 4, 1, 0, bias=False)\n",
    "        self.actvn = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        batch_size = x.shape[0]\n",
    "        if x.shape[1] != self.in_dim:\n",
    "            x = x[:, :self.in_dim]\n",
    "        for layer in self.blocks:\n",
    "            x = self.actvn(layer(x))\n",
    "\n",
    "        out = self.conv_out(x)\n",
    "        out = out.reshape(batch_size, 1)\n",
    "        return out\n",
    "\n",
    "discriminator = DCDiscriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the generator\n",
    "There are 4 main component of the generator:\n",
    "- decoder\n",
    "- background_generator\n",
    "- bounding_box_generator\n",
    "- neural_renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    ''' Decoder class.\n",
    "\n",
    "    Predicts volume density and color from 3D location, viewing\n",
    "    direction, and latent code z.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): hidden size of Decoder network\n",
    "        n_blocks (int): number of layers\n",
    "        n_blocks_view (int): number of view-dep layers\n",
    "        skips (list): where to add a skip connection\n",
    "        use_viewdirs: (bool): whether to use viewing directions\n",
    "        n_freq_posenc (int), max freq for positional encoding (3D location)\n",
    "        n_freq_posenc_views (int), max freq for positional encoding (\n",
    "            viewing direction)\n",
    "        dim (int): input dimension\n",
    "        z_dim (int): dimension of latent code z\n",
    "        rgb_out_dim (int): output dimension of feature / rgb prediction\n",
    "        final_sigmoid_activation (bool): whether to apply a sigmoid activation\n",
    "            to the feature / rgb output\n",
    "        downscale_by (float): downscale factor for input points before applying\n",
    "            the positional encoding\n",
    "        positional_encoding (str): type of positional encoding\n",
    "        gauss_dim_pos (int): dim for Gauss. positional encoding (position)\n",
    "        gauss_dim_view (int): dim for Gauss. positional encoding (\n",
    "            viewing direction)\n",
    "        gauss_std (int): std for Gauss. positional encoding\n",
    "    '''\n",
    "\n",
    "    def __init__(self, hidden_size=128, n_blocks=8, n_blocks_view=1,\n",
    "                 skips=[4], use_viewdirs=True, n_freq_posenc=10,\n",
    "                 n_freq_posenc_views=4,\n",
    "                 z_dim=64, rgb_out_dim=128, final_sigmoid_activation=False,\n",
    "                 downscale_p_by=2., positional_encoding=\"normal\",\n",
    "                 gauss_dim_pos=10, gauss_dim_view=4, gauss_std=4.,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.use_viewdirs = use_viewdirs\n",
    "        self.n_freq_posenc = n_freq_posenc\n",
    "        self.n_freq_posenc_views = n_freq_posenc_views\n",
    "        self.skips = skips\n",
    "        self.downscale_p_by = downscale_p_by\n",
    "        self.z_dim = z_dim\n",
    "        self.final_sigmoid_activation = final_sigmoid_activation\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_blocks_view = n_blocks_view\n",
    "\n",
    "        assert(positional_encoding in ('normal', 'gauss'))\n",
    "        self.positional_encoding = positional_encoding\n",
    "        if positional_encoding == 'gauss':\n",
    "            np.random.seed(42)\n",
    "            # remove * 2 because of cos and sin\n",
    "            self.B_pos = gauss_std * \\\n",
    "                torch.from_numpy(np.random.randn(\n",
    "                    1,  gauss_dim_pos * 3, 3)).float().cuda()\n",
    "            self.B_view = gauss_std * \\\n",
    "                torch.from_numpy(np.random.randn(\n",
    "                    1,  gauss_dim_view * 3, 3)).float().cuda()\n",
    "            dim_embed = 3 * gauss_dim_pos * 2\n",
    "            dim_embed_view = 3 * gauss_dim_view * 2\n",
    "        else:\n",
    "            dim_embed = 3 * self.n_freq_posenc * 2\n",
    "            dim_embed_view = 3 * self.n_freq_posenc_views * 2\n",
    "\n",
    "        # Density Prediction Layers\n",
    "        self.fc_in = nn.Linear(dim_embed, hidden_size)\n",
    "        if z_dim > 0:\n",
    "            self.fc_z = nn.Linear(z_dim, hidden_size)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for i in range(n_blocks - 1)\n",
    "        ])\n",
    "        n_skips = sum([i in skips for i in range(n_blocks - 1)])\n",
    "        if n_skips > 0:\n",
    "            self.fc_z_skips = nn.ModuleList(\n",
    "                [nn.Linear(z_dim, hidden_size) for i in range(n_skips)]\n",
    "            )\n",
    "            self.fc_p_skips = nn.ModuleList([\n",
    "                nn.Linear(dim_embed, hidden_size) for i in range(n_skips)\n",
    "            ])\n",
    "        self.sigma_out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Feature Prediction Layers\n",
    "        self.fc_z_view = nn.Linear(z_dim, hidden_size)\n",
    "        self.feat_view = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_view = nn.Linear(dim_embed_view, hidden_size)\n",
    "        self.feat_out = nn.Linear(hidden_size, rgb_out_dim)\n",
    "        if use_viewdirs and n_blocks_view > 1:\n",
    "            self.blocks_view = nn.ModuleList(\n",
    "                [nn.Linear(dim_embed_view + hidden_size, hidden_size)\n",
    "                 for i in range(n_blocks_view - 1)])\n",
    "\n",
    "    def transform_points(self, p, views=False):\n",
    "        # Positional encoding\n",
    "        # normalize p between [-1, 1]\n",
    "        p = p / self.downscale_p_by\n",
    "\n",
    "        # we consider points up to [-1, 1]\n",
    "        # so no scaling required here\n",
    "        if self.positional_encoding == 'gauss':\n",
    "            B = self.B_view if views else self.B_pos\n",
    "            p_transformed = (B @ (pi * p.permute(0, 2, 1))).permute(0, 2, 1)\n",
    "            p_transformed = torch.cat(\n",
    "                [torch.sin(p_transformed), torch.cos(p_transformed)], dim=-1)\n",
    "        else:\n",
    "            L = self.n_freq_posenc_views if views else self.n_freq_posenc\n",
    "            p_transformed = torch.cat([torch.cat(\n",
    "                [torch.sin((2 ** i) * pi * p),\n",
    "                 torch.cos((2 ** i) * pi * p)],\n",
    "                dim=-1) for i in range(L)], dim=-1)\n",
    "        return p_transformed\n",
    "\n",
    "    def forward(self, p_in, ray_d, z_shape=None, z_app=None, **kwargs):\n",
    "        a = F.relu\n",
    "        if self.z_dim > 0:\n",
    "            batch_size = p_in.shape[0]\n",
    "            if z_shape is None:\n",
    "                z_shape = torch.randn(batch_size, self.z_dim).to(p_in.device)\n",
    "            if z_app is None:\n",
    "                z_app = torch.randn(batch_size, self.z_dim).to(p_in.device)\n",
    "        p = self.transform_points(p_in)\n",
    "        net = self.fc_in(p)\n",
    "        if z_shape is not None:\n",
    "            net = net + self.fc_z(z_shape).unsqueeze(1)\n",
    "        net = a(net)\n",
    "\n",
    "        skip_idx = 0\n",
    "        for idx, layer in enumerate(self.blocks):\n",
    "            net = a(layer(net))\n",
    "            if (idx + 1) in self.skips and (idx < len(self.blocks) - 1):\n",
    "                net = net + self.fc_z_skips[skip_idx](z_shape).unsqueeze(1)\n",
    "                net = net + self.fc_p_skips[skip_idx](p)\n",
    "                skip_idx += 1\n",
    "        sigma_out = self.sigma_out(net).squeeze(-1)\n",
    "\n",
    "        net = self.feat_view(net)\n",
    "        net = net + self.fc_z_view(z_app).unsqueeze(1)\n",
    "        if self.use_viewdirs and ray_d is not None:\n",
    "            ray_d = ray_d / torch.norm(ray_d, dim=-1, keepdim=True)\n",
    "            ray_d = self.transform_points(ray_d, views=True)\n",
    "            net = net + self.fc_view(ray_d)\n",
    "            net = a(net)\n",
    "            if self.n_blocks_view > 1:\n",
    "                for layer in self.blocks_view:\n",
    "                    net = a(layer(net))\n",
    "        feat_out = self.feat_out(net)\n",
    "\n",
    "        if self.final_sigmoid_activation:\n",
    "            feat_out = torch.sigmoid(feat_out)\n",
    "\n",
    "        return feat_out, sigma_out\n",
    "\n",
    "decoder = Decoder(z_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundingBoxGenerator(nn.Module):\n",
    "    ''' Bounding box generator class\n",
    "\n",
    "    Args:\n",
    "        n_boxes (int): number of bounding boxes (excluding background)\n",
    "        scale_range_min (list): min scale values for x, y, z\n",
    "        scale_range_max (list): max scale values for x, y, z\n",
    "        translation_range_min (list): min values for x, y, z translation\n",
    "        translation_range_max (list): max values for x, y, z translation\n",
    "        z_level_plane (float): value of z-plane; only relevant if\n",
    "            object_on_plane is set True\n",
    "        rotation_range (list): min and max rotation value (between 0 and 1)\n",
    "        check_collision (bool): whether to check for collisions\n",
    "        collision_padding (float): padding for collision checking\n",
    "        fix_scale_ratio (bool): whether the x/y/z scale ratio should be fixed\n",
    "        object_on_plane (bool): whether the objects should be placed on a plane\n",
    "            with value z_level_plane\n",
    "        prior_npz_file (str): path to prior npz file (used for clevr) to sample\n",
    "            locations from\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_boxes=1,\n",
    "                 scale_range_min=[0.5, 0.5, 0.5],\n",
    "                 scale_range_max=[0.5, 0.5, 0.5],\n",
    "                 translation_range_min=[-0.75, -0.75, 0.],\n",
    "                 translation_range_max=[0.75, 0.75, 0.],\n",
    "                 z_level_plane=0., rotation_range=[0., 1.],\n",
    "                 check_collison=False, collision_padding=0.1,\n",
    "                 fix_scale_ratio=True, object_on_plane=False,\n",
    "                 prior_npz_file=None, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_boxes = n_boxes\n",
    "        self.scale_min = torch.tensor(scale_range_min).reshape(1, 1, 3)\n",
    "        self.scale_range = (torch.tensor(scale_range_max) -\n",
    "                            torch.tensor(scale_range_min)).reshape(1, 1, 3)\n",
    "\n",
    "        self.translation_min = torch.tensor(\n",
    "            translation_range_min).reshape(1, 1, 3)\n",
    "        self.translation_range = (torch.tensor(\n",
    "            translation_range_max) - torch.tensor(translation_range_min)\n",
    "        ).reshape(1, 1, 3)\n",
    "\n",
    "        self.z_level_plane = z_level_plane\n",
    "        self.rotation_range = rotation_range\n",
    "        self.check_collison = check_collison\n",
    "        self.collision_padding = collision_padding\n",
    "        self.fix_scale_ratio = fix_scale_ratio\n",
    "        self.object_on_plane = object_on_plane\n",
    "\n",
    "        if prior_npz_file is not None:\n",
    "            try:\n",
    "                prior = np.load(prior_npz_file)['coordinates']\n",
    "                # We multiply by ~0.23 as this is multiplier of the original clevr\n",
    "                # world and our world scale\n",
    "                self.prior = torch.from_numpy(prior).float() * 0.2378777237835723\n",
    "            except Exception as e: \n",
    "                print(\"WARNING: Clevr prior location file could not be loaded!\")\n",
    "                print(\"For rendering, this is fine, but for training, please download the files using the download script.\")\n",
    "                self.prior = None\n",
    "        else:\n",
    "            self.prior = None\n",
    "\n",
    "    def check_for_collison(self, s, t):\n",
    "        n_boxes = s.shape[1]\n",
    "        if n_boxes == 1:\n",
    "            is_free = torch.ones_like(s[..., 0]).bool().squeeze(1)\n",
    "        elif n_boxes == 2:\n",
    "            d_t = (t[:, :1] - t[:, 1:2]).abs()\n",
    "            d_s = (s[:, :1] + s[:, 1:2]).abs() + self.collision_padding\n",
    "            is_free = (d_t >= d_s).any(-1).squeeze(1)\n",
    "        elif n_boxes == 3:\n",
    "            is_free_1 = self.check_for_collison(s[:, [0, 1]], t[:, [0, 1]])\n",
    "            is_free_2 = self.check_for_collison(s[:, [0, 2]], t[:, [0, 2]])\n",
    "            is_free_3 = self.check_for_collison(s[:, [1, 2]], t[:, [1, 2]])\n",
    "            is_free = is_free_1 & is_free_2 & is_free_3\n",
    "        else:\n",
    "            print(\"ERROR: Not implemented\")\n",
    "        return is_free\n",
    "\n",
    "    def get_translation(self, batch_size=32, val=[[0.5, 0.5, 0.5]]):\n",
    "        n_boxes = len(val)\n",
    "        t = self.translation_min + \\\n",
    "            torch.tensor(val).reshape(1, n_boxes, 3) * self.translation_range\n",
    "        t = t.repeat(batch_size, 1, 1)\n",
    "        if self.object_on_plane:\n",
    "            t[..., -1] = self.z_level_plane\n",
    "        return t\n",
    "\n",
    "    def get_rotation(self, batch_size=32, val=[0.]):\n",
    "        r_range = self.rotation_range\n",
    "        values = [r_range[0] + v * (r_range[1] - r_range[0]) for v in val]\n",
    "        r = torch.cat([get_rotation_matrix(\n",
    "            value=v, batch_size=batch_size).unsqueeze(1) for v in values],\n",
    "            dim=1)\n",
    "        r = r.float()\n",
    "        return r\n",
    "\n",
    "    def get_scale(self, batch_size=32, val=[[0.5, 0.5, 0.5]]):\n",
    "        n_boxes = len(val)\n",
    "        if self.fix_scale_ratio:\n",
    "            t = self.scale_min + \\\n",
    "                torch.tensor(val).reshape(\n",
    "                    1, n_boxes, -1)[..., :1] * self.scale_range\n",
    "        else:\n",
    "            t = self.scale_min + \\\n",
    "                torch.tensor(val).reshape(1, n_boxes, 3) * self.scale_range\n",
    "        t = t.repeat(batch_size, 1, 1)\n",
    "        return t\n",
    "\n",
    "    def get_random_offset(self, batch_size):\n",
    "        n_boxes = self.n_boxes\n",
    "        # Sample sizes\n",
    "        if self.fix_scale_ratio:\n",
    "            s_rand = torch.rand(batch_size, n_boxes, 1)\n",
    "        else:\n",
    "            s_rand = torch.rand(batch_size, n_boxes, 3)\n",
    "        s = self.scale_min + s_rand * self.scale_range\n",
    "\n",
    "        # Sample translations\n",
    "        if self.prior is not None:\n",
    "            idx = np.random.randint(self.prior.shape[0], size=(batch_size))\n",
    "            t = self.prior[idx]\n",
    "        else:\n",
    "            t = self.translation_min + \\\n",
    "                torch.rand(batch_size, n_boxes, 3) * self.translation_range\n",
    "            if self.check_collison:\n",
    "                is_free = self.check_for_collison(s, t)\n",
    "                while not torch.all(is_free):\n",
    "                    t_new = self.translation_min + \\\n",
    "                        torch.rand(batch_size, n_boxes, 3) * \\\n",
    "                        self.translation_range\n",
    "                    t[is_free == 0] = t_new[is_free == 0]\n",
    "                    is_free = self.check_for_collison(s, t)\n",
    "            if self.object_on_plane:\n",
    "                t[..., -1] = self.z_level_plane\n",
    "\n",
    "        def r_val(): return self.rotation_range[0] + np.random.rand() * (\n",
    "            self.rotation_range[1] - self.rotation_range[0])\n",
    "        R = [torch.from_numpy(\n",
    "            Rot.from_euler('z', r_val() * 2 * np.pi).as_dcm())\n",
    "            for i in range(batch_size * self.n_boxes)]\n",
    "        R = torch.stack(R, dim=0).reshape(\n",
    "            batch_size, self.n_boxes, -1).cuda().float()\n",
    "        return s, t, R\n",
    "\n",
    "    def forward(self, batch_size=32):\n",
    "        s, t, R = self.get_random_offset(batch_size)\n",
    "        R = R.reshape(batch_size, self.n_boxes, 3, 3)\n",
    "        return s, t, R\n",
    "    \n",
    "    \n",
    "bounding_box_generator = BoundingBoxGenerator(z_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralRenderer(nn.Module):\n",
    "    ''' Neural renderer class\n",
    "\n",
    "    Args:\n",
    "        n_feat (int): number of features\n",
    "        input_dim (int): input dimension; if not equal to n_feat,\n",
    "            it is projected to n_feat with a 1x1 convolution\n",
    "        out_dim (int): output dimension\n",
    "        final_actvn (bool): whether to apply a final activation (sigmoid)\n",
    "        min_feat (int): minimum features\n",
    "        img_size (int): output image size\n",
    "        use_rgb_skip (bool): whether to use RGB skip connections\n",
    "        upsample_feat (str): upsampling type for feature upsampling\n",
    "        upsample_rgb (str): upsampling type for rgb upsampling\n",
    "        use_norm (bool): whether to use normalization\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "            self, n_feat=128, input_dim=128, out_dim=3, final_actvn=True,\n",
    "            min_feat=32, img_size=64, use_rgb_skip=True,\n",
    "            upsample_feat=\"nn\", upsample_rgb=\"bilinear\", use_norm=False,\n",
    "            **kwargs):\n",
    "        super().__init__()\n",
    "        self.final_actvn = final_actvn\n",
    "        self.input_dim = input_dim\n",
    "        self.use_rgb_skip = use_rgb_skip\n",
    "        self.use_norm = use_norm\n",
    "        n_blocks = int(log2(img_size) - 4)\n",
    "\n",
    "        assert(upsample_feat in (\"nn\", \"bilinear\"))\n",
    "        if upsample_feat == \"nn\":\n",
    "            self.upsample_2 = nn.Upsample(scale_factor=2.)\n",
    "        elif upsample_feat == \"bilinear\":\n",
    "            self.upsample_2 = nn.Sequential(nn.Upsample(\n",
    "                scale_factor=2, mode='bilinear', align_corners=False), Blur())\n",
    "\n",
    "        assert(upsample_rgb in (\"nn\", \"bilinear\"))\n",
    "        if upsample_rgb == \"nn\":\n",
    "            self.upsample_rgb = nn.Upsample(scale_factor=2.)\n",
    "        elif upsample_rgb == \"bilinear\":\n",
    "            self.upsample_rgb = nn.Sequential(nn.Upsample(\n",
    "                scale_factor=2, mode='bilinear', align_corners=False), Blur())\n",
    "\n",
    "        if n_feat == input_dim:\n",
    "            self.conv_in = lambda x: x\n",
    "        else:\n",
    "            self.conv_in = nn.Conv2d(input_dim, n_feat, 1, 1, 0)\n",
    "\n",
    "        self.conv_layers = nn.ModuleList(\n",
    "            [nn.Conv2d(n_feat, n_feat // 2, 3, 1, 1)] +\n",
    "            [nn.Conv2d(max(n_feat // (2 ** (i + 1)), min_feat),\n",
    "                       max(n_feat // (2 ** (i + 2)), min_feat), 3, 1, 1)\n",
    "                for i in range(0, n_blocks - 1)]\n",
    "        )\n",
    "        if use_rgb_skip:\n",
    "            self.conv_rgb = nn.ModuleList(\n",
    "                [nn.Conv2d(input_dim, out_dim, 3, 1, 1)] +\n",
    "                [nn.Conv2d(max(n_feat // (2 ** (i + 1)), min_feat),\n",
    "                           out_dim, 3, 1, 1) for i in range(0, n_blocks)]\n",
    "            )\n",
    "        else:\n",
    "            self.conv_rgb = nn.Conv2d(\n",
    "                max(n_feat // (2 ** (n_blocks)), min_feat), 3, 1, 1)\n",
    "\n",
    "        if use_norm:\n",
    "            self.norms = nn.ModuleList([\n",
    "                nn.InstanceNorm2d(max(n_feat // (2 ** (i + 1)), min_feat))\n",
    "                for i in range(n_blocks)\n",
    "            ])\n",
    "        self.actvn = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        net = self.conv_in(x)\n",
    "\n",
    "        if self.use_rgb_skip:\n",
    "            rgb = self.upsample_rgb(self.conv_rgb[0](x))\n",
    "\n",
    "        for idx, layer in enumerate(self.conv_layers):\n",
    "            hid = layer(self.upsample_2(net))\n",
    "            if self.use_norm:\n",
    "                hid = self.norms[idx](hid)\n",
    "            net = self.actvn(hid)\n",
    "\n",
    "            if self.use_rgb_skip:\n",
    "                rgb = rgb + self.conv_rgb[idx + 1](net)\n",
    "                if idx < len(self.conv_layers) - 1:\n",
    "                    rgb = self.upsample_rgb(rgb)\n",
    "\n",
    "        if not self.use_rgb_skip:\n",
    "            rgb = self.conv_rgb(net)\n",
    "\n",
    "        if self.final_actvn:\n",
    "            rgb = torch.sigmoid(rgb)\n",
    "        return rgb\n",
    "\n",
    "\n",
    "neural_renderer = NeuralRenderer(z_dim=256, img_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
